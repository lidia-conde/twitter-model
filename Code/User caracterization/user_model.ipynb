{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to generate the agent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from distfit import distfit\n",
    "import math\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DIR = './'\n",
    "os.chdir(PATH_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets with the clustered users\n",
    "PATH = 'Data/Original dataset 11-2022/'\n",
    "FILE_NAME = 'clustered_users_'\n",
    "\n",
    "df = {}\n",
    "\n",
    "for i in range(4):\n",
    "    df[str(i)] = pd.read_excel(PATH+FILE_NAME+str(i)+'.xlsx')\n",
    "\n",
    "# Dataset with the selected distributions for each attribute\n",
    "PATH = 'Data/'\n",
    "FILE_NAME = 'selected_distributions.xlsx'\n",
    "\n",
    "ds_distr = pd.read_excel(PATH+FILE_NAME)\n",
    "ds_distr = ds_distr.set_index(['CLUSTER','ATTRIBUTE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distribution(dataset, distr=None, bins='auto', method=0):\n",
    "\n",
    "    # Initialize distfit\n",
    "    if (method==0):\n",
    "        dist = distfit(distr=distr, bins=bins)\n",
    "    else:   # binary\n",
    "        dist = distfit(method=method)\n",
    "\n",
    "    # Determine best-fitting probability distribution for data\n",
    "    dist.fit_transform(dataset, verbose=0)\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_attributes = ['friends_count', 'followers_count', 'listed_count', 'favourites_count', 'statuses_count', 'created_at']\n",
    "binary_attributes = ['verified', 'location', 'protected', 'geo_enabled', 'default_profile', 'default_profile_image']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "\n",
    "# integer attributes\n",
    "for cluster, dataset in df.items():\n",
    "    model_dict[cluster] = {}\n",
    "    for x in int_attributes:\n",
    "        data = dataset[x]\n",
    "        ds = ds_distr.loc[int(cluster), x]\n",
    "\n",
    "        model = find_distribution(data, distr=ds['distr'])\n",
    "\n",
    "        model_dict[cluster][x] = model\n",
    "    # binary attributes\n",
    "    for x in binary_attributes:\n",
    "        data = dataset[x]\n",
    "        ds = ds_distr.loc[int(cluster), x]\n",
    "        n = ds['n']\n",
    "\n",
    "        if (math.isnan(n)==False):\n",
    "            p = 1\n",
    "        else:\n",
    "            model = find_distribution(data, method='discrete')\n",
    "            n = model.model['n']\n",
    "            p = model.model['p']\n",
    "        \n",
    "        model_dict[cluster][x] = {'n':n, 'p':p}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "PIK = \"model_final.dat\"\n",
    "\n",
    "with open(PIK, \"wb\") as f:\n",
    "    pickle.dump(model_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new data based on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, n_samples=1000):\n",
    "    Xgenerate = model.generate(n=n_samples)\n",
    "    return Xgenerate\n",
    "\n",
    "def generate_binary_samples(n, p, n_samples=1000):\n",
    "    return np.random.binomial(n, p, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated model\n",
    "PATH = \"D:/TFG/TFG Code/Datasets/Datasets/New generated users/\"\n",
    "PIK = \"model_final.dat\"\n",
    "\n",
    "with open(PATH+PIK, \"rb\") as f:\n",
    "    model_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = {}\n",
    "total_samples = 100000\n",
    "percentages = {'0': 0.04653327128897162 / 100, '1': 87.79989167664716 / 100, '2': 1.3532790699448465 / 100, '3': 10.800295982119017 / 100}\n",
    "\n",
    "#for cluster in df.keys():\n",
    "for cluster in model_dict.keys():\n",
    "    generated_data[cluster] = {}\n",
    "    n_samples = int(total_samples * percentages[cluster])\n",
    "\n",
    "    ids = [cluster+'x'+str(i) for i in range(n_samples)]\n",
    "\n",
    "    generated_data[cluster]['id'] = np.array(ids)\n",
    "    \n",
    "    # integer attributes\n",
    "    for x in int_attributes[:-1]:\n",
    "        model = model_dict[cluster][x]\n",
    "\n",
    "        new_data = generate_samples(model, n_samples=n_samples)\n",
    "\n",
    "        new_data = new_data[new_data >= 0]\n",
    "        \n",
    "        while (len(new_data) < n_samples):\n",
    "            aux_data = generate_samples(model, n_samples=n_samples-len(new_data))\n",
    "            aux_data = aux_data[aux_data >= 0]\n",
    "            new_data = np.concatenate((new_data, aux_data))\n",
    "        \n",
    "        new_data = np.rint(new_data)\n",
    "\n",
    "        generated_data[cluster][x] = new_data\n",
    "\n",
    "    # created_at\n",
    "    for x in [int_attributes[-1]]:\n",
    "        model = model_dict[cluster][x]\n",
    "\n",
    "        new_data = generate_samples(model, n_samples=n_samples)\n",
    "\n",
    "        new_data = new_data[(new_data >= 2006) & (new_data <= 2023)]\n",
    "        \n",
    "        while (len(new_data) < n_samples):\n",
    "            aux_data = generate_samples(model, n_samples=n_samples-len(new_data))\n",
    "            aux_data = aux_data[(aux_data >= 2006) & (aux_data <= 2023)]\n",
    "            new_data = np.concatenate((new_data, aux_data))\n",
    "        \n",
    "        new_data = new_data.astype(np.int64)\n",
    "\n",
    "        generated_data[cluster][x] = new_data\n",
    "            \n",
    "    # binary attributes\n",
    "    for x in binary_attributes:\n",
    "        model = model_dict[cluster][x]\n",
    "        \n",
    "        new_data = generate_binary_samples(model['n'], model['p'], n_samples=n_samples)\n",
    "        \n",
    "        generated_data[cluster][x] = new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the synthetic users\n",
    "PATH = 'Data/Generated users/'\n",
    "FILE_NAME = 'generated_users_'\n",
    "\n",
    "for cluster in generated_data.keys():\n",
    "    generated_cluster = pd.DataFrame(generated_data[cluster])\n",
    "    generated_cluster.to_excel(PATH+FILE_NAME+cluster+'.xlsx', engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'Data/Generated users/'\n",
    "FILE_NAME = 'user_model'\n",
    "\n",
    "with open(PATH+FILE_NAME+'.dat', \"wb\") as f:\n",
    "    pickle.dump(generated_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
